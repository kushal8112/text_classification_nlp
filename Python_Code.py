# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10MKtT1yJVJsCvT_sfZjL2MCANmbRP4eA

# Megaminds IT Services Assignment

### This code is implemented as per the standards of research paper: "Multi-Class Text Classification: Model Comparison and Selection".

Text mining (TM) is increasingly vital with the rise in electronic documents. Text classification frameworks, including Naive Bayes, Linear Support Vector Machine, Logistic Regression, Word2Vec, Doc2Vec, and BOW with Keras, are commonly employed for effective document categorization.This paper compares these methods to determine the optimal machine learning model for multi-class text classification, addressing the question: "Which model is most suitable for a given problem?". For this, they have implemented multiple models like:


1.   Naive Bayes Classifier (Multinomial Models)
2.   Linear Support Vector Machine
3.   Logistic Regression
4.   Word2Vec (Logistic Regression)
5.   Logistic Doc2Vc
6.   BOW with Keras

And to make the results better, I have implemented two models:

1.   BERT (Bidirectional Encoder Representations from Trans-formers)
2.   RoBERTa (Robustly Optimized BERT approach)

I have diligently implemented various machine learning models outlined in the paper, while also addressing research gaps by introducing novel models and methods of my own.
"""

!pip install -q opendatasets
!pip install nltk
!pip install transformers[torch]
!pip install datasets
!pip install huggingface_hub
!sudo apt-get install git-lfs --yes

# Commented out IPython magic to ensure Python compatibility.
import opendatasets as od
import pandas as pd
import copy
import torch
import string
import numpy as np
import seaborn as sns
import nltk
import gensim
import spacy
import itertools
import os
import tensorflow as tf
# %matplotlib inline
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
nltk.download('stopwords')
from datasets import load_dataset
from transformers import (
    RobertaTokenizerFast,
    RobertaForSequenceClassification,
    TrainingArguments,
    Trainer,
    AutoConfig,
)
from tqdm import tqdm
tqdm.pandas(desc="progress-bar")
from gensim.models import Doc2Vec
from sklearn import utils
from gensim.models.doc2vec import TaggedDocument
import re
import gensim
import matplotlib.pyplot as plt
import datasets
from datasets import Dataset
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout
from keras.preprocessing import text, sequence
from keras import utils
from gensim.models import word2vec
from gensim.models.word2vec import Word2Vec
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelBinarizer, LabelEncoder
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB

"""## Importing the Dataset

To overcome the research gaps, i have implemented BERT (Bidirectional Encoder Representations from Trans-formers) and RoBERTa (Robustly Optimized BERT approach) models. I chose BERT and RoBERTa because these pre-trained models undergo training on vast text datasets, encompassing books, articles, and websites. Their ability to be fine-tuned for specific NLP tasks, including text classification, makes them well-suited for my goal.

RoBERTa takes a longer time to train on the dataset, therefore, I have pushed the trained model into my Huggingface account. To check for the results of RoBERTa model, just running the evaulation part is enough. For BERT model, it must be trained.
"""

from huggingface_hub import HfFolder, notebook_login
!huggingface-cli login --token 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
model_id = "roberta-base"
repository_id = "V12X-ksr/FOCALtrain"

"""The dataset is from Kaggle, named: top-20-play-store-app-reviews-daily-update. The Dataset consists of 10000 latest reviews from the Top 20 apps on the Google Play Store. My laptop cannot handle a big dataset, therefore I have just imported Candy Crush Saga.csv file, which consists of 10000 reviews. I have imported the dataset using opendatasets library, for which it will ask Kaggle user_id and key. Alternatively, you can import the dataset by uploading it to the colab/jupyter files section. I have commented that code."""

od.download('https://www.kaggle.com/datasets/odins0n/top-20-play-store-app-reviews-daily-update') # insert you kaggle username and key
df = pd.read_csv('/content/top-20-play-store-app-reviews-daily-update/Candy Crush Saga.csv')
stopwords_from_nltk = set(stopwords.words('english'))

# df = pd.read_csv('/content/Candy Crush Saga.csv')
# stopwords_from_nltk = set(stopwords.words('english'))

"""## Pre-processing the Dataset"""

df = df.rename(columns={'content':'content', 'score':'score'})
df1=df.copy()
df.dropna(axis=1,inplace=True)
df.info()
df['score'].value_counts()
def text_cleaning(a):
 cleaning = [char for char in a if char not in string.punctuation]
 cleaning=''.join(cleaning)
 return [word for word in cleaning.split() if word.lower() not in stopwords_from_nltk]
df1['content'] = df1['content'].map(text_cleaning)
df1['content'] = df1['content'].astype(str)

"""## 1. Naive Bayes Classifier (Multinomial Models)"""

X_train, X_test, y_train, y_test = train_test_split(df1['content'], df['score'], test_size=0.2, random_state=50)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfTransformer

nb = Pipeline([('vect', CountVectorizer()),
               ('tfidf', TfidfTransformer()),
               ('clf', MultinomialNB()),
              ])
nb.fit(X_train, y_train)

from sklearn.metrics import classification_report
y_pred = nb.predict(X_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred))

"""## 2. Linear Support Vector Machine"""

from sklearn.linear_model import SGDClassifier

sgd = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),
               ])
sgd.fit(X_train, y_train)

y_pred = sgd.predict(X_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred))

"""## 3. Logistic Regression"""

from sklearn.linear_model import LogisticRegression

logreg = Pipeline([('vect', CountVectorizer()),
                ('tfidf', TfidfTransformer()),
                ('clf', LogisticRegression(n_jobs=1, C=1e5)),
               ])
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred))

"""## 4. Word2Vec (Logistic Regression)"""

import gensim.downloader as api
print(list(gensim.downloader.info()['models'].keys()))
wv = api.load('glove-twitter-50')
wv.save('/content/sample_data/vectors.kv')

"""### Word2Vec Vectorization"""

def sent_vec(sent):
    vector_size = wv.vector_size
    wv_res = np.zeros(vector_size)
    # print(wv_res)
    ctr = 1
    for w in sent:
        if w in wv:
            ctr += 1
            wv_res += wv[w]
    wv_res = wv_res/ctr
    return wv_res

"""### Dataset Cleaning"""

def spacy_tokenizer(sentence):
    doc = nlp(sentence)
    mytokens = [ word.lemma_.lower().strip() for word in doc ]
    mytokens = [ word for word in mytokens if word not in stopwords_from_spacy]
    return mytokens

nlp = spacy.load("en_core_web_sm")
stopwords_from_spacy = nlp.Defaults.stop_words
print(stopwords_from_spacy)

df1['content'] = df1['content'].apply(spacy_tokenizer)
df1['content'] = df1['content'].apply(sent_vec)
X = df1['content'].to_list()
y = df1['score'].to_list()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y)

classifier = LogisticRegression()
classifier.fit(X_train,y_train)
from sklearn import metrics
predicted = classifier.predict(X_test)
print("Logistic Regression Accuracy:",metrics.accuracy_score(predicted, y_test))
print("Logistic Regression Report:",metrics.classification_report(y_test, predicted))

"""
##5. Logistic Doc2Vc
### Doc2vec and Logistic Regression
"""

def label_sentences(corpus, label_type):
    labeled = []
    for i, v in enumerate(corpus):
        label = label_type + '_' + str(i)
        labeled.append(TaggedDocument(v.split(), [label]))
    return labeled
X_train, X_test, y_train, y_test = train_test_split(df['content'], df['score'], random_state=0, test_size=0.3)
X_train = label_sentences(X_train, 'Train')
X_test = label_sentences(X_test, 'Test')
all_data = X_train + X_test

model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)
model_dbow.build_vocab([x for x in tqdm(all_data)])

for epoch in range(10):
    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)
    model_dbow.alpha -= 0.002
    model_dbow.min_alpha = model_dbow.alpha

def get_vectors(model, corpus_size, vectors_size, vectors_type):
    vectors = np.zeros((corpus_size, vectors_size))
    for i in range(0, corpus_size):
        prefix = vectors_type + '_' + str(i)
        vectors[i] = model.docvecs[prefix]
    return vectors

train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')
test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')

logreg = LogisticRegression(n_jobs=1, C=1e5)
logreg.fit(train_vectors_dbow, y_train)
logreg = logreg.fit(train_vectors_dbow, y_train)
y_pred = logreg.predict(test_vectors_dbow)
print('accuracy %s' % accuracy_score(y_pred, y_test))
print(classification_report(y_test, y_pred))

"""## 6. BOW (Bag-Of-Words) with Keras"""

df1['content'] = df1["content"].astype(str)

train_size = int(len(df1) * .8)
train_posts = df1['content'][:train_size]
train_tags = df1['score'][:train_size]

test_posts = df1['content'][train_size:]
test_tags = df1['score'][train_size:]

max_words = 128
tokenize = text.Tokenizer(num_words=max_words, char_level=False)
tokenize.fit_on_texts(train_posts) # only fit on train

x_train = tokenize.texts_to_matrix(train_posts)
x_test = tokenize.texts_to_matrix(test_posts)

encoder = LabelEncoder()
encoder.fit(train_tags)
y_train = encoder.transform(train_tags)
y_test = encoder.transform(test_tags)

num_classes = np.max(y_train) + 1
y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)

batch_size = 32
epochs = 10

# Build the model
model = Sequential()
model.add(Dense(512, input_shape=(max_words,)))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(num_classes))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

history = model.fit(x_train, y_train,
                    batch_size=batch_size,
                    epochs=epochs,
                    verbose=1,
                    validation_split=0.1)

predicted_vector = model.predict(x_test)
predicted_values = []
classes = [1,2,3,4,5]
for i in range(len(predicted_vector)):
  predicted_values.append(classes[np.argmax(predicted_vector[i])])
print(accuracy_score(predicted_values,test_tags))
print(classification_report(test_tags,predicted_values))

"""## 1. BERT (Bidirectional Encoder Representations from Trans-formers)"""

!pip install transformers # importing the transformers

from tqdm.auto import tqdm
import tensorflow as tf
from transformers import BertTokenizer

df1 = df.drop("reviewId", axis=1)
Functions = df1.rename(columns={'content':'Functions Text','score':'Functions Label'})
Functions["Functions Label"] = Functions["Functions Label"] - 1
Test_Functions = Functions.iloc[9001:9999,:]
Functions = Functions.iloc[0:9000,:]
labels = [1,2,3,4,5]
x = 9000
xx = 998
l = 128
labe = [0,1,2,3,4]

X_input_ids = np.zeros((x , l))
X_attn_masks = np.zeros((x , l))

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
def generate_training_data(d, ids, masks, tokenizer):
    for i, text in tqdm(enumerate(d["Functions Text"])):
        tokenized_text = tokenizer.encode_plus(
            text,
            max_length=l,
            truncation=True,
            padding='max_length',
            add_special_tokens=True,
            return_tensors='tf'
        )
        ids[i, :] = tokenized_text.input_ids
        masks[i, :] = tokenized_text.attention_mask
    return ids, masks

X_input_ids, X_attn_masks = generate_training_data(Functions, X_input_ids, X_attn_masks, tokenizer)

labels = np.zeros((x, 5))
labels[np.arange(x), Functions["Functions Label"].values] = 1

dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))
def LabelDatasetMapFunction(input_ids, attn_masks, labels):
    return {'input_ids': input_ids,'attention_mask': attn_masks}, labels
dataset = dataset.map(LabelDatasetMapFunction)
dataset = dataset.shuffle(10000).batch(2, drop_remainder=True)
p = 0.8
train_size = int((x/2)*p)
train_dataset = dataset.take(train_size)
val_dataset = dataset.skip(train_size)
val_dataset

from transformers import TFAutoModel
model = TFAutoModel.from_pretrained('bert-base-cased')
model.summary()

input_ids = tf.keras.layers.Input(shape=(l,), name='input_ids', dtype='int32')
attn_masks = tf.keras.layers.Input(shape=(l,), name='attention_mask', dtype='int32')

bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1] # 0 -> activation layer (3D), 1 -> pooled output layer (2D)
intermediate_layer = tf.keras.layers.Dense(1024, activation='relu', name='intermediate_layer')(bert_embds)
output_layer = tf.keras.layers.Dense(5, activation='softmax', name='output_layer')(intermediate_layer) # softmax -> calcs probs of classes

label_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)
label_model.summary()

optim = tf.keras.optimizers.legacy.Adam(learning_rate=1e-5, decay=1e-6)
loss_func = tf.keras.losses.CategoricalCrossentropy()
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')

label_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])

hist = label_model.fit(train_dataset, validation_data=val_dataset, epochs = 5)
label_model.save('label_model')

label_model = tf.keras.models.load_model('label_model')

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

def prepare_data(input_text, tokenizer):
    token = tokenizer.encode_plus(
        input_text,
        max_length=128,
        truncation=True,
        padding='max_length',
        add_special_tokens=True,
        return_tensors='tf'
    )
    return {
        'input_ids': tf.cast(token.input_ids, tf.float64),
        'attention_mask': tf.cast(token.attention_mask, tf.float64)
    }

def make_prediction(model, processed_data, classes=[1,2,3,4,5]):
  finall=[]
  for i in processed_data:
    probs = model.predict(i)[0]
    finall.append(classes[np.argmax(probs)])
  return finall

dup = np.ones((xx,1))
fnc = Test_Functions['Functions Text'].copy()
fnclab = pd.DataFrame(dup,columns=["Functions Label"])
g=[]
for i in range(0,len(fnc)):
  input_text = fnc.iloc[i]
  processed_data = prepare_data(input_text, tokenizer)
  g.append(processed_data)

predicted_outcome = make_prediction(label_model,processed_data = g)

print(accuracy_score(predicted_outcome,(Test_Functions['Functions Label']+1)))
print(classification_report((Test_Functions['Functions Label']+1),predicted_outcome))

"""## 2. RoBERTa (Robustly Optimized BERT approach)"""

Functions = df1.rename(columns={'content':'text','score':'label'})
Functions["label"] = Functions["label"] - 1
Test_Functions = Functions.iloc[9001:9999,:]
Functions = Functions.iloc[0:9000,:]
labels = [1,2,3,4,5]
x = 9000
xx = 998
l = 128
labe = [0,1,2,3,4]

train_dataset1= Functions.iloc[0:8000,:]
val_dataset1 = Functions.iloc[8001:8999,:]
test_dataset1 = Test_Functions

train_dataset = Dataset.from_pandas(train_dataset1[["text","label"]])
val_dataset = Dataset.from_pandas(val_dataset1[["text","label"]])
test_dataset = Dataset.from_pandas(test_dataset1[["text","label"]])

tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')
def tokenize(batch):
    return tokenizer(batch["text"], padding=True, truncation=True, max_length=512)

train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))
val_dataset = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))
test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))

train_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
val_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])
test_dataset.set_format("torch", columns=["input_ids", "attention_mask", "label"])

num_labels = 5
class_names = [1,2,3,4,5]
id2label = {i: label for i, label in enumerate(class_names)}
config = AutoConfig.from_pretrained(model_id)
config.update({"id2label": id2label})

model = RobertaForSequenceClassification.from_pretrained(model_id, config=config)

# TrainingArguments
training_args = TrainingArguments(
    output_dir=repository_id,
    num_train_epochs=5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy="epoch",
    logging_dir=f"{repository_id}/logs",
    logging_strategy="steps",
    logging_steps=10,
    learning_rate=5e-5,
    weight_decay=0.01,
    warmup_steps=500,
    save_strategy="epoch",
    load_best_model_at_end=True,
    save_total_limit=2,
    report_to="tensorboard",
    push_to_hub=True,
    hub_strategy="every_save",
    hub_model_id=repository_id,
    hub_token=HfFolder.get_token(),
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

trainer.evaluate()

tokenizer.save_pretrained(repository_id)
trainer.create_model_card()
trainer.push_to_hub()

from transformers import pipeline
classifier = pipeline('text-classification',repository_id)

fnc = Test_Functions['text']
predicted_outcome=[]
for i in range(0,len(fnc)):
  try:
    result = classifier(fnc.iloc[i])
    predicted_label = result[0]["label"]
    predicted_outcome.append(predicted_label)
  except RuntimeError:
    predicted_outcome.append(5)
print(accuracy_score(predicted_outcome,Test_Functions['label']+1))
print(classification_report(Test_Functions['label']+1,predicted_outcome))